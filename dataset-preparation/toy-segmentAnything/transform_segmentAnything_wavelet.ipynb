{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT_DIR = Path(git.Repo('.', search_parent_directories=True).working_tree_dir)\n",
    "\n",
    "DATASET = \"segmentAnything\"\n",
    "FINAL_DATA_NAME = 'segmentAnything-toy-wavelet-horizontal' # + channel\n",
    "CONSTANT_SAMPLE_SIZE = int(1e5)\n",
    "RAW_DATA_SUFFIX = \"segmentAnything-resizedBlurred-normalized\"\n",
    "SAVE_DF = False\n",
    "\n",
    "data_dir = os.path.join(ROOT_DIR, 'raw-data','segmentAnything')\n",
    "file_list = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir)]\n",
    "file_names = os.listdir(data_dir)\n",
    "data_dir\n",
    "BATCH_NUM = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(ROOT_DIR, \"utilities\"))\n",
    "from transform import *\n",
    "os.chdir(os.path.join(ROOT_DIR, \"dataset-preparation\"))\n",
    "freq_df = pd.read_csv(os.path.join(ROOT_DIR, \"transformed-data\", f\"master-frequency-map.csv\"), index_col= [\"dataset\", \"transform\", \"group\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sa_1192.npz', 'sa_3565.npz', 'sa_3922.npz', 'sa_7865.npz', 'sa_3799.npz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = [os.path.join(data_dir, f\"mini-toy-{RAW_DATA_SUFFIX}\", filename) for filename in os.listdir(data_dir)]\n",
    "file_names = os.listdir(os.path.join(data_dir, f\"mini-toy-{RAW_DATA_SUFFIX}\"))\n",
    "file_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assuming No batching is required. Not applicable for agriVision'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Assuming No batching is required. Not applicable for agriVision'''\n",
    "\n",
    "# data_dir = os.path.join(ROOT_DIR, \"raw-data\", \"agriVision\", \"full-agriVision-RGB-cleaned\")\n",
    "\n",
    "# for channel in ['red', 'blue', 'green', 'gray', 'infrared']:\n",
    "\n",
    "#     channel_fr = convert_to_fourier_basis(data_dir, channel, debug = True)\n",
    "#     pd.to_pickle(channel_fr, os.path.join(ROOT_DIR, \"transformed-data\", f\"full-agriVision-fourier-{channel}-df.pickle\"))\n",
    "\n",
    "#     min_group, max_group = 2, max(channel_fr['band'])\n",
    "#     group_data_map = dict()\n",
    "#     group_data_map_size = dict()\n",
    "#     for group in np.arange(min_group, max_group + 1):\n",
    "#         data = channel_fr[(channel_fr['band'] == group)]['data'].iloc[0]\n",
    "#         group_data_map[group] = np.sort(data)[np.round(np.linspace(0, data.size - 1, min(data.size, CONSTANT_SAMPLE_SIZE))).astype(int)] \n",
    "#         group_data_map_size[group] = data.size\n",
    "    \n",
    "#     pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME}-{channel}.pickle\"))\n",
    "#     pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME}-{channel}-size.pickle\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To split large dataset into many batches, only needs to be run once'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To split large dataset into many batches, only needs to be run once'''\n",
    "# k = 10000\n",
    "# target_dir = os.path.join(ROOT_DIR, 'raw-data', 'agriVision') # Where the batch{i} folders will be created\n",
    "# directorySplit(folder_dir = data_dir, target_dir = target_dir, name = RAW_DATA_SUFFIX, k = k)\n",
    "# print(f\"{len(file_names)//k} batches created\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toy-segmentAnything-resizedBlurred-normalized\n",
      "._toy-segmentAnything-resizedBlurred-normalized\n",
      "mini4k-toy-segmentAnything-resizedBlurred-normalized\n",
      "mini-toy-segmentAnything-resizedBlurred-normalized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Show all subsets of data in raw data folder that have already been created'''\n",
    "print(''.join([x+\"\\n\" for x in os.listdir(data_dir) if x.__contains__(RAW_DATA_SUFFIX)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npz_opener_pickle(path):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DATA_NAME = 'segmentAnything-toy-wavelet'\n",
    "if BATCH_NUM is None:\n",
    "    batch_dir = os.path.join(ROOT_DIR, \"raw-data\", \"segmentAnything\", f\"mini-toy-{RAW_DATA_SUFFIX}\")\n",
    "else:\n",
    "    batch_dir = os.path.join(ROOT_DIR, \"raw-data\", \"segmentAnything\", f\"batch{BATCH_NUM}-{RAW_DATA_SUFFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 layers being used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:06<00:00, 30.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing H\n",
      "{np.int64(2): array([-828.55334, -777.9856 , -768.7299 , ...,  775.6144 ,  790.81   ,\n",
      "        936.8076 ], shape=(2000,), dtype=float32), np.int64(3): array([-491.78625, -487.4613 , -476.7782 , ...,  476.96347,  485.96426,\n",
      "        486.88596], shape=(8000,), dtype=float32), np.int64(4): array([-317.74973, -277.5523 , -277.29834, ...,  283.5554 ,  312.4923 ,\n",
      "        320.78683], shape=(32000,), dtype=float32), np.int64(5): array([-166.25873, -166.1253 , -164.81894, ...,  171.96962,  177.49973,\n",
      "        177.79565], shape=(100000,), dtype=float32), np.int64(6): array([-98.31728 , -81.374115, -79.26432 , ...,  78.6924  ,  82.86543 ,\n",
      "        90.43092 ], shape=(100000,), dtype=float32), np.int64(7): array([-45.28487 , -40.257282, -38.95217 , ...,  40.623047,  43.125202,\n",
      "        50.056427], shape=(100000,), dtype=float32), np.int64(8): array([-24.556046, -20.54927 , -19.61491 , ...,  19.727964,  20.585846,\n",
      "        24.0473  ], shape=(100000,), dtype=float32), np.int64(9): array([-12.795079, -10.514044, -10.101728, ...,   9.982024,  10.460842,\n",
      "        12.682025], shape=(100000,), dtype=float32), np.int64(10): array([-6.743353 , -5.732515 , -5.466505 , ...,  5.3601007,  5.6394114,\n",
      "        6.6768503], shape=(100000,), dtype=float32)}\n",
      "10 layers being used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing V\n",
      "{np.int64(2): array([-617.8318 , -617.3472 , -563.61774, ...,  669.82025,  680.73285,\n",
      "        705.4555 ], shape=(2000,), dtype=float32), np.int64(3): array([-422.30756, -382.0876 , -377.53787, ...,  407.73395,  509.1729 ,\n",
      "        521.9833 ], shape=(8000,), dtype=float32), np.int64(4): array([-308.50195, -268.4269 , -241.42255, ...,  245.71071,  249.75987,\n",
      "        251.7161 ], shape=(32000,), dtype=float32), np.int64(5): array([-166.88718, -166.1066 , -156.64787, ...,  160.39569,  161.56781,\n",
      "        161.84462], shape=(100000,), dtype=float32), np.int64(6): array([-81.427315, -78.0972  , -77.0581  , ...,  78.83871 ,  81.60604 ,\n",
      "        88.538925], shape=(100000,), dtype=float32), np.int64(7): array([-46.267445, -40.75439 , -39.206543, ...,  38.987087,  41.48758 ,\n",
      "        47.637398], shape=(100000,), dtype=float32), np.int64(8): array([-23.372301, -20.509369, -19.45863 , ...,  19.428703,  20.299885,\n",
      "        24.492868], shape=(100000,), dtype=float32), np.int64(9): array([-12.748528, -10.261334,  -9.795817, ...,   9.669462,  10.081778,\n",
      "        12.329562], shape=(100000,), dtype=float32), np.int64(10): array([-6.783254 , -5.5995097, -5.3335   , ...,  5.2403965,  5.4798055,\n",
      "        6.730052 ], shape=(100000,), dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "TRANSFORM = \"wavelet\"\n",
    "channel = \"red\"\n",
    "\n",
    "for orientation_label in ['horizontal', 'vertical']:\n",
    "    FINAL_DATA_NAME_ORIENTED = f\"{FINAL_DATA_NAME}-{orientation_label}\"\n",
    "    orientation_code = 'H' if orientation_label == 'horizontal' else 'V'\n",
    "\n",
    "    channel_wv = convert_to_wavelet_basis(batch_dir, channel, debug=True, image_opener=npz_opener)\n",
    "    channel_wv['data'] = channel_wv['data'].apply(lambda x: x.astype(np.float32))\n",
    "\n",
    "    if SAVE_DF:\n",
    "        if BATCH_NUM is None:\n",
    "            pd.to_pickle(channel_wv, os.path.join(ROOT_DIR, \"transformed-data\", f\"dataframes/{FINAL_DATA_NAME_ORIENTED}-{channel}-df.pickle\"))\n",
    "        else:\n",
    "            pd.to_pickle(channel_wv, os.path.join(ROOT_DIR, \"transformed-data\", f\"dataframes/batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}-df.pickle\"))\n",
    "\n",
    "    min_group, max_group = 2, max(channel_wv['layer'])\n",
    "    group_data_map = dict()\n",
    "    group_data_map_size = dict()\n",
    "\n",
    "    for group in np.arange(min_group, max_group + 1):\n",
    "        data = channel_wv[(channel_wv['orientation'] == orientation_code) & (channel_wv['layer'] == group)]['data'].iloc[0]\n",
    "        group_data_map[group] = np.sort(data)[np.round(np.linspace(0, data.size - 1, min(data.size, CONSTANT_SAMPLE_SIZE))).astype(int)]\n",
    "        group_data_map_size[group] = data.size\n",
    "\n",
    "        freq_df.loc[DATASET, TRANSFORM, group] = channel_wv[(channel_wv['layer'] == group) & (channel_wv['orientation'] == orientation_code)]['frequency'].iloc[0]\n",
    "\n",
    "    print(f\"printing {orientation_code}\")\n",
    "    freq_df.to_csv(os.path.join(ROOT_DIR, \"transformed-data\", \"master-frequency-map.csv\"))\n",
    "\n",
    "    if BATCH_NUM is None:\n",
    "        pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME_ORIENTED}-{channel}.pickle\"))\n",
    "        pd.to_pickle(group_data_map_size, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME_ORIENTED}-{channel}-size.pickle\"))\n",
    "    else:\n",
    "        pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}.pickle\"))\n",
    "        pd.to_pickle(group_data_map_size, os.path.join(ROOT_DIR, \"transformed-data\", f\"batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}-size.pickle\"))\n",
    "\n",
    "    print(group_data_map)\n",
    "\n",
    "    del channel_wv\n",
    "    del group_data_map\n",
    "    del group_data_map_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 layers being used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:10<00:00, 28.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing H\n",
      "{np.int64(2): array([-887.8562 , -843.32776, -758.5118 , ...,  774.7783 ,  927.0954 ,\n",
      "        974.49615], shape=(2000,), dtype=float32), np.int64(3): array([-491.3666 , -490.6369 , -466.63266, ...,  541.56323,  541.74774,\n",
      "        584.66364], shape=(8000,), dtype=float32), np.int64(4): array([-367.69183, -327.91757, -324.7943 , ...,  334.65918,  344.1061 ,\n",
      "        344.50693], shape=(32000,), dtype=float32), np.int64(5): array([-196.94052, -194.46771, -182.09634, ...,  188.79276,  192.39694,\n",
      "        194.32458], shape=(100000,), dtype=float32), np.int64(6): array([-112.50255 ,  -95.018   ,  -93.69904 , ...,   92.26968 ,\n",
      "         93.802605,  106.2917  ], shape=(100000,), dtype=float32), np.int64(7): array([-53.776512, -48.33653 , -46.452854, ...,  48.197792,  49.676983,\n",
      "        59.04454 ], shape=(100000,), dtype=float32), np.int64(8): array([-28.938997, -24.925446, -23.557632, ...,  23.54591 ,  24.51901 ,\n",
      "        28.868652], shape=(100000,), dtype=float32), np.int64(9): array([-15.163175, -12.497895, -11.98985 , ...,  11.942954,  12.505711,\n",
      "        15.194439], shape=(100000,), dtype=float32), np.int64(10): array([-7.9411263, -6.79998  , -6.4873376, ...,  6.3622804,  6.6749234,\n",
      "        7.9723907], shape=(100000,), dtype=float32)}\n",
      "10 layers being used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing V\n",
      "{np.int64(2): array([-650.8697 , -647.53217, -629.22845, ...,  714.1395 ,  733.45496,\n",
      "        740.5868 ], shape=(2000,), dtype=float32), np.int64(3): array([-468.45355, -455.6865 , -427.29608, ...,  493.72253,  510.34195,\n",
      "        588.08167], shape=(8000,), dtype=float32), np.int64(4): array([-348.16266, -290.35428, -283.76385, ...,  287.36072,  299.31564,\n",
      "        302.46576], shape=(32000,), dtype=float32), np.int64(5): array([-182.70355, -180.40074, -176.26262, ...,  193.09697,  197.5746 ,\n",
      "        205.2451 ], shape=(100000,), dtype=float32), np.int64(6): array([-99.6754  , -93.12749 , -91.18324 , ...,  94.663345,  98.400406,\n",
      "       108.13532 ], shape=(100000,), dtype=float32), np.int64(7): array([-54.58352 , -48.287678, -46.128487, ...,  46.0855  ,  48.62572 ,\n",
      "        56.463284], shape=(100000,), dtype=float32), np.int64(8): array([-28.759228, -24.343147, -23.291887, ...,  23.08476 ,  23.975792,\n",
      "        28.946814], shape=(100000,), dtype=float32), np.int64(9): array([-15.468001, -12.2087  , -11.700656, ...,  11.51307 ,  11.997666,\n",
      "        14.678578], shape=(100000,), dtype=float32), np.int64(10): array([-7.9723907, -6.6749234, -6.346648 , ...,  6.221591 ,  6.5029697,\n",
      "        7.909862 ], shape=(100000,), dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "TRANSFORM = \"wavelet\"\n",
    "channel = \"green\"\n",
    "\n",
    "for orientation_label in ['horizontal', 'vertical']:\n",
    "    FINAL_DATA_NAME_ORIENTED = f\"{FINAL_DATA_NAME}-{orientation_label}\"\n",
    "    orientation_code = 'H' if orientation_label == 'horizontal' else 'V'\n",
    "\n",
    "    channel_wv = convert_to_wavelet_basis(batch_dir, channel, debug=True, image_opener=npz_opener)\n",
    "    channel_wv['data'] = channel_wv['data'].apply(lambda x: x.astype(np.float32))\n",
    "\n",
    "    if SAVE_DF:\n",
    "        if BATCH_NUM is None:\n",
    "            pd.to_pickle(channel_wv, os.path.join(ROOT_DIR, \"transformed-data\", f\"dataframes/{FINAL_DATA_NAME_ORIENTED}-{channel}-df.pickle\"))\n",
    "        else:\n",
    "            pd.to_pickle(channel_wv, os.path.join(ROOT_DIR, \"transformed-data\", f\"dataframes/batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}-df.pickle\"))\n",
    "\n",
    "    min_group, max_group = 2, max(channel_wv['layer'])\n",
    "    group_data_map = dict()\n",
    "    group_data_map_size = dict()\n",
    "\n",
    "    for group in np.arange(min_group, max_group + 1):\n",
    "        data = channel_wv[(channel_wv['orientation'] == orientation_code) & (channel_wv['layer'] == group)]['data'].iloc[0]\n",
    "        group_data_map[group] = np.sort(data)[np.round(np.linspace(0, data.size - 1, min(data.size, CONSTANT_SAMPLE_SIZE))).astype(int)]\n",
    "        group_data_map_size[group] = data.size\n",
    "\n",
    "        freq_df.loc[DATASET, TRANSFORM, group] = channel_wv[(channel_wv['layer'] == group) & (channel_wv['orientation'] == orientation_code)]['frequency'].iloc[0]\n",
    "\n",
    "    print(f\"printing {orientation_code}\")\n",
    "    freq_df.to_csv(os.path.join(ROOT_DIR, \"transformed-data\", \"master-frequency-map.csv\"))\n",
    "\n",
    "    if BATCH_NUM is None:\n",
    "        pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME_ORIENTED}-{channel}.pickle\"))\n",
    "        pd.to_pickle(group_data_map_size, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME_ORIENTED}-{channel}-size.pickle\"))\n",
    "    else:\n",
    "        pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}.pickle\"))\n",
    "        pd.to_pickle(group_data_map_size, os.path.join(ROOT_DIR, \"transformed-data\", f\"batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}-size.pickle\"))\n",
    "\n",
    "    print(group_data_map)\n",
    "\n",
    "    del channel_wv\n",
    "    del group_data_map\n",
    "    del group_data_map_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 layers being used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:06<00:00, 29.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing H\n",
      "{np.int64(2): array([-772.83  , -765.4451, -763.1141, ...,  717.9474,  782.2642,\n",
      "       1035.2745], shape=(2000,), dtype=float32), np.int64(3): array([-563.2518 , -504.9972 , -469.08127, ...,  478.35974,  531.23755,\n",
      "        554.7824 ], shape=(8000,), dtype=float32), np.int64(4): array([-379.6117 , -321.90048, -315.689  , ...,  293.42908,  301.57712,\n",
      "        337.38736], shape=(32000,), dtype=float32), np.int64(5): array([-179.7652 , -175.84215, -165.52469, ...,  166.15033,  172.78447,\n",
      "        189.34677], shape=(100000,), dtype=float32), np.int64(6): array([-97.23192, -90.2482 , -88.22941, ...,  83.32602,  87.67813,\n",
      "        94.908  ], shape=(100000,), dtype=float32), np.int64(7): array([-46.656143, -43.53821 , -42.61172 , ...,  43.721115,  44.876663,\n",
      "        50.541588], shape=(100000,), dtype=float32), np.int64(8): array([-25.227205, -22.471666, -21.357141, ...,  20.998169,  21.86654 ,\n",
      "        25.299   ], shape=(100000,), dtype=float32), np.int64(9): array([-13.415307 , -10.919596 , -10.481991 , ...,  10.461478 ,\n",
      "        10.981133 ,  12.9093275], shape=(100000,), dtype=float32), np.int64(10): array([-6.9469686, -5.9213333, -5.6615057, ...,  5.56578  ,  5.8392825,\n",
      "        6.960644 ], shape=(100000,), dtype=float32)}\n",
      "10 layers being used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:09<00:00, 28.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing V\n",
      "{np.int64(2): array([-848.7576 , -667.5582 , -613.92584, ...,  697.5296 ,  709.3108 ,\n",
      "        733.74457], shape=(2000,), dtype=float32), np.int64(3): array([-487.10007, -454.3274 , -406.25726, ...,  452.57452,  501.36975,\n",
      "        501.43793], shape=(8000,), dtype=float32), np.int64(4): array([-329.99106, -327.27502, -279.76526, ...,  318.29196,  325.396  ,\n",
      "        337.48993], shape=(32000,), dtype=float32), np.int64(5): array([-159.90677, -156.11534, -154.56407, ...,  167.5875 ,  171.84175,\n",
      "        174.56865], shape=(100000,), dtype=float32), np.int64(6): array([-94.57467 , -88.40889 , -86.109764, ...,  86.0619  ,  89.745636,\n",
      "        99.992584], shape=(100000,), dtype=float32), np.int64(7): array([-48.31938 , -44.02026 , -41.890358, ...,  42.237362,  43.85616 ,\n",
      "        47.615112], shape=(100000,), dtype=float32), np.int64(8): array([-24.379347, -21.534918, -20.615267, ...,  20.225525,  21.162271,\n",
      "        25.206692], shape=(100000,), dtype=float32), np.int64(9): array([-13.545221 , -10.65293  , -10.229001 , ...,  10.058062 ,\n",
      "        10.488829 ,  13.0050535], shape=(100000,), dtype=float32), np.int64(10): array([-6.974319 , -5.8119326, -5.5384297, ...,  5.4290285,  5.675181 ,\n",
      "        6.919618 ], shape=(100000,), dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "TRANSFORM = \"wavelet\"\n",
    "channel = \"blue\"\n",
    "\n",
    "for orientation_label in ['horizontal', 'vertical']:\n",
    "    FINAL_DATA_NAME_ORIENTED = f\"{FINAL_DATA_NAME}-{orientation_label}\"\n",
    "    orientation_code = 'H' if orientation_label == 'horizontal' else 'V'\n",
    "\n",
    "    channel_wv = convert_to_wavelet_basis(batch_dir, channel, debug=True, image_opener=npz_opener)\n",
    "    channel_wv['data'] = channel_wv['data'].apply(lambda x: x.astype(np.float32))\n",
    "\n",
    "    if SAVE_DF:\n",
    "        if BATCH_NUM is None:\n",
    "            pd.to_pickle(channel_wv, os.path.join(ROOT_DIR, \"transformed-data\", f\"dataframes/{FINAL_DATA_NAME_ORIENTED}-{channel}-df.pickle\"))\n",
    "        else:\n",
    "            pd.to_pickle(channel_wv, os.path.join(ROOT_DIR, \"transformed-data\", f\"dataframes/batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}-df.pickle\"))\n",
    "\n",
    "    min_group, max_group = 2, max(channel_wv['layer'])\n",
    "    group_data_map = dict()\n",
    "    group_data_map_size = dict()\n",
    "\n",
    "    for group in np.arange(min_group, max_group + 1):\n",
    "        data = channel_wv[(channel_wv['orientation'] == orientation_code) & (channel_wv['layer'] == group)]['data'].iloc[0]\n",
    "        group_data_map[group] = np.sort(data)[np.round(np.linspace(0, data.size - 1, min(data.size, CONSTANT_SAMPLE_SIZE))).astype(int)]\n",
    "        group_data_map_size[group] = data.size\n",
    "\n",
    "        freq_df.loc[DATASET, TRANSFORM, group] = channel_wv[(channel_wv['layer'] == group) & (channel_wv['orientation'] == orientation_code)]['frequency'].iloc[0]\n",
    "\n",
    "    print(f\"printing {orientation_code}\")\n",
    "    freq_df.to_csv(os.path.join(ROOT_DIR, \"transformed-data\", \"master-frequency-map.csv\"))\n",
    "\n",
    "    if BATCH_NUM is None:\n",
    "        pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME_ORIENTED}-{channel}.pickle\"))\n",
    "        pd.to_pickle(group_data_map_size, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME_ORIENTED}-{channel}-size.pickle\"))\n",
    "    else:\n",
    "        pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}.pickle\"))\n",
    "        pd.to_pickle(group_data_map_size, os.path.join(ROOT_DIR, \"transformed-data\", f\"batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}-size.pickle\"))\n",
    "\n",
    "    print(group_data_map)\n",
    "\n",
    "    del channel_wv\n",
    "    del group_data_map\n",
    "    del group_data_map_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 layers being used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:01<00:00, 32.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing H\n",
      "{np.int64(2): array([-829.9487 , -812.94025, -707.7685 , ...,  729.36505,  903.2977 ,\n",
      "        926.42365], shape=(2000,), dtype=float32), np.int64(3): array([-472.2292 , -454.1876 , -436.49988, ...,  487.21234,  511.92654,\n",
      "        549.0133 ], shape=(8000,), dtype=float32), np.int64(4): array([-347.50717, -305.07553, -298.34656, ...,  304.11325,  319.56082,\n",
      "        328.84515], shape=(32000,), dtype=float32), np.int64(5): array([-175.93819, -168.60172, -168.18925, ...,  181.03627,  182.42055,\n",
      "        182.53221], shape=(100000,), dtype=float32), np.int64(6): array([-104.40869,  -88.57438,  -86.35828, ...,   85.33793,   87.22171,\n",
      "         99.93777], shape=(100000,), dtype=float32), np.int64(7): array([-49.880054, -44.1313  , -42.472702, ...,  44.51042 ,  46.495   ,\n",
      "        55.38275 ], shape=(100000,), dtype=float32), np.int64(8): array([-27.105087, -23.092379, -21.80031 , ...,  21.828737,  22.740433,\n",
      "        26.60471 ], shape=(100000,), dtype=float32), np.int64(9): array([-14.153202, -11.679986, -11.186609, ...,  11.100784,  11.677421,\n",
      "        14.181457], shape=(100000,), dtype=float32), np.int64(10): array([-7.4650083, -6.360492 , -6.075457 , ...,  5.9620814,  6.26361  ,\n",
      "        7.442904 ], shape=(100000,), dtype=float32)}\n",
      "10 layers being used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1338/2000 [00:58<00:30, 21.64it/s]"
     ]
    }
   ],
   "source": [
    "TRANSFORM = \"wavelet\"\n",
    "channel = \"gray\"\n",
    "\n",
    "for orientation_label in ['horizontal', 'vertical']:\n",
    "    FINAL_DATA_NAME_ORIENTED = f\"{FINAL_DATA_NAME}-{orientation_label}\"\n",
    "    orientation_code = 'H' if orientation_label == 'horizontal' else 'V'\n",
    "\n",
    "    channel_wv = convert_to_wavelet_basis(batch_dir, channel, debug=True, image_opener=npz_opener)\n",
    "    channel_wv['data'] = channel_wv['data'].apply(lambda x: x.astype(np.float32))\n",
    "\n",
    "    if SAVE_DF:\n",
    "        if BATCH_NUM is None:\n",
    "            pd.to_pickle(channel_wv, os.path.join(ROOT_DIR, \"transformed-data\", f\"dataframes/{FINAL_DATA_NAME_ORIENTED}-{channel}-df.pickle\"))\n",
    "        else:\n",
    "            pd.to_pickle(channel_wv, os.path.join(ROOT_DIR, \"transformed-data\", f\"dataframes/batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}-df.pickle\"))\n",
    "\n",
    "    min_group, max_group = 2, max(channel_wv['layer'])\n",
    "    group_data_map = dict()\n",
    "    group_data_map_size = dict()\n",
    "\n",
    "    for group in np.arange(min_group, max_group + 1):\n",
    "        data = channel_wv[(channel_wv['orientation'] == orientation_code) & (channel_wv['layer'] == group)]['data'].iloc[0]\n",
    "        group_data_map[group] = np.sort(data)[np.round(np.linspace(0, data.size - 1, min(data.size, CONSTANT_SAMPLE_SIZE))).astype(int)]\n",
    "        group_data_map_size[group] = data.size\n",
    "\n",
    "        freq_df.loc[DATASET, TRANSFORM, group] = channel_wv[(channel_wv['layer'] == group) & (channel_wv['orientation'] == orientation_code)]['frequency'].iloc[0]\n",
    "\n",
    "    print(f\"printing {orientation_code}\")\n",
    "    freq_df.to_csv(os.path.join(ROOT_DIR, \"transformed-data\", \"master-frequency-map.csv\"))\n",
    "\n",
    "    if BATCH_NUM is None:\n",
    "        pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME_ORIENTED}-{channel}.pickle\"))\n",
    "        pd.to_pickle(group_data_map_size, os.path.join(ROOT_DIR, \"transformed-data\", f\"{FINAL_DATA_NAME_ORIENTED}-{channel}-size.pickle\"))\n",
    "    else:\n",
    "        pd.to_pickle(group_data_map, os.path.join(ROOT_DIR, \"transformed-data\", f\"batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}.pickle\"))\n",
    "        pd.to_pickle(group_data_map_size, os.path.join(ROOT_DIR, \"transformed-data\", f\"batch{BATCH_NUM}{FINAL_DATA_NAME_ORIENTED}-{channel}-size.pickle\"))\n",
    "\n",
    "    print(group_data_map)\n",
    "\n",
    "    del channel_wv\n",
    "    del group_data_map\n",
    "    del group_data_map_size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hbmv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
