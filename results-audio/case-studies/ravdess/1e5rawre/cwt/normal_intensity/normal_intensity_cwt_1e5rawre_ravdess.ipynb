{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, Loading Data and CDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'ravdess-1e5rawre' #dataset-compression\n",
    "TRANSFORM = 'cwt-normal_intensity' #transform-group\n",
    "CHANNEL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = DATA_NAME.split(\"-\") + TRANSFORM.split(\"-\")\n",
    "if CHANNEL:\n",
    "    path_list.append(CHANNEL)\n",
    "print(f\"Name the notebook:\\n{'_'.join(path_list[::-1])}.ipynb\")\n",
    "FULL_DATA_NAME='-'.join(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT_DIR = Path(git.Repo('.', search_parent_directories=True).working_tree_dir)\n",
    "CWD = os.path.join(ROOT_DIR, \"results-audio\", \"case-studies\", *path_list)\n",
    "\n",
    "assert CWD == os.getcwd()\n",
    "Path(os.path.join(CWD, \"CSVs\")).mkdir(exist_ok=True)\n",
    "Path(os.path.join(CWD, \"plots\")).mkdir(exist_ok=True)\n",
    "Path(os.path.join(CWD, \"cache\")).mkdir(exist_ok=True)\n",
    "\n",
    "GROUP = 'band' \n",
    "RERUN = False\n",
    "SKIP_OPTIMIZE_STEP = False\n",
    "CWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(ROOT_DIR, \"utilities\"))\n",
    "from testing import * # If MATLAB is not installed, open utilities and set to False\n",
    "from plotting import *\n",
    "os.chdir(CWD)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data_map = pd.read_pickle(os.path.join(ROOT_DIR, \"transformed-data-audio\", \"subsample-data\", f'{FULL_DATA_NAME}.pickle'))\n",
    "group_total_samples = pd.read_pickle(os.path.join(ROOT_DIR, \"transformed-data-audio\", \"subsample-data\", f'{FULL_DATA_NAME}-size.pickle'))\n",
    "NUM_BANDS = int(10) #Change to len(group_data_map) to use all bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'erb' in TRANSFORM:\n",
    "    group_data_map.popitem()\n",
    "    print(group_data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if 'fourier' in TRANSFORM:\n",
    "#    GROUPS = np.arange(2, sorted(group_data_map)[-1] + 1)[::3]\n",
    "#elif 'wavelet' in TRANSFORM:\n",
    "#    GROUPS = np.arange(2, sorted(group_data_map)[-1] + 1)\n",
    "#GROUPS = np.arange(2, sorted(group_data_map)[-1] + 1)[::max(len(group_data_map) // NUM_BANDS, 1)]\n",
    "GROUPS = np.linspace(2, sorted(group_data_map)[-1], NUM_BANDS, dtype=int)\n",
    "\n",
    "\n",
    "cdfs_dir = os.path.join(ROOT_DIR, \"results\", \"CDFs\")\n",
    "cdfs_list = sorted([os.path.join(cdfs_dir, i) for i in os.listdir(cdfs_dir)])\n",
    "all_cdfs = combine_pickles(cdfs_list[0])\n",
    "for cdf_dir in cdfs_list[:6]: # TODO For quick testing purposes. Currently excluding 20000 CDFs. Use cdfs_list[5:6] to access CDFs/_test\n",
    "    all_cdfs = all_cdfs | combine_pickles(cdf_dir)\n",
    "    \n",
    "# group_data_map = {g : group_data_map[g][::100] for g in GROUPS} # TODO For quick testing purposes\n",
    "#group_total_samples\n",
    "#group_data_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "all_cdfs_df = create_kurt_var_ksstat_df(all_cdfs)\n",
    "fine_cdfs_df = all_cdfs_df.copy()\n",
    "all_cdfs_df = all_cdfs_df[(np.round(all_cdfs_df['r'], 1) == all_cdfs_df['r']) & (np.round(all_cdfs_df['eta'], 1) == all_cdfs_df['eta'])]\n",
    "all_cdfs_df = all_cdfs_df.reset_index(drop=True)\n",
    "all_cdfs = {x[0] : x[1] for x in all_cdfs_df[\"(r,eta),cdf\"]}\n",
    "                                             \n",
    "var_values_dict = dict()\n",
    "kurt_values_dict = dict()\n",
    "master_df = pd.DataFrame(columns=[GROUP]).set_index(GROUP)\n",
    "temp_cdf = all_cdfs_df\n",
    "print(f\"Running {all_cdfs_df.shape[0]} CDFs\")\n",
    "create_scatter_plot(all_cdfs_df);\n",
    "all_cdfs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the Variance and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = int(1e5) # TODO For quick testing purposes\n",
    "bootstrap_size = int(1e4) # TODO For quick testing purposes\n",
    "ci = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_path = Path(os.path.join(CWD, \"CSVs\", f'initial_grid_bootstrap{n_bootstrap}_{bootstrap_size}_ci{ci}.csv'))\n",
    "master_df_var_kurt_path = Path(os.path.join(CWD, \"CSVs\", f'initial_grid_bootstrap{n_bootstrap}_{bootstrap_size}_ci{ci}.csv'))\n",
    "\n",
    "if RERUN or not master_df_var_kurt_path.exists(): # TODO For quick testing purposes\n",
    "    for group in GROUPS:\n",
    "        print(f'{GROUP.capitalize()} {group}')\n",
    "        obs_var, var_lower, var_upper, var_values_dict[group] = bootstrap_metric(group_data_map[group], \n",
    "                                                                                n_bootstrap=n_bootstrap, \n",
    "                                                                                bootstrap_size=min(group_data_map[group].size, bootstrap_size), \n",
    "                                                                                metric= np.var, \n",
    "                                                                                ci=ci)\n",
    "        obs_kurt, kurt_lower, kurt_upper, kurt_values_dict[group] = bootstrap_metric(group_data_map[group], \n",
    "                                                                                    n_bootstrap=n_bootstrap, \n",
    "                                                                                    bootstrap_size=min(group_data_map[group].size, bootstrap_size), \n",
    "                                                                                    metric= stats.kurtosis, ci=ci)  \n",
    "        master_df.loc[group, 'obs_var'], master_df.loc[group, 'var_lower'], master_df.loc[group, 'var_upper'] = obs_var, var_lower, var_upper\n",
    "        master_df.loc[group, 'obs_kurt'], master_df.loc[group, 'kurt_lower'], master_df.loc[group, 'kurt_upper'] = obs_kurt, kurt_lower, kurt_upper\n",
    "        master_df.loc[group, 'total_samples'] = group_total_samples[group]\n",
    "\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", f'initial_grid_bootstrap{n_bootstrap}_{bootstrap_size}_ci{ci}.csv'))\n",
    "\n",
    "master_df = pd.read_csv(master_df_var_kurt_path, index_col=GROUP)\n",
    "var_kurt_df = pd.read_csv(bootstrap_path, index_col=GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Grid Search and Hypothesis Test Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_CUTOFFS = [0, 25, 50, 75, 100, 150, 200, 250, 300, 350, 500] # TODO For quick testing purposes\n",
    "SCALE_CAP = 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_var_df_path = Path(os.path.join(CWD, \"CSVs\", \"augmented_var_df.csv\"))\n",
    "augmented_var_df = var_kurt_df[['obs_var']].copy()\n",
    "augmented_var_df\n",
    "for val in EXCLUDE_CUTOFFS:\n",
    "    temp_arr = []\n",
    "    for group in GROUPS:\n",
    "        if val != 0:\n",
    "            temp_arr.append(np.var(group_data_map[group][val:-val]))\n",
    "        else:\n",
    "            temp_arr.append(np.var(group_data_map[group]))\n",
    "\n",
    "    augmented_var_df[f\"var_exclude_{val}\"] = temp_arr\n",
    "augmented_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_combo_path = Path(os.path.join(CWD, \"CSVs\", \"master_df_combo.csv\"))\n",
    "rEtaKsstats_dict_path = Path(os.path.join(CWD, \"cache\", \"rEtaKsstats_dict.pickle\"))\n",
    "\n",
    "rEtaKsstats_dict = dict()\n",
    "\n",
    "if RERUN or not master_df_combo_path.exists():\n",
    "    sorted_params = sorted(all_cdfs)\n",
    "    ksstats_dict = dict()\n",
    "\n",
    "    for i, group in enumerate(GROUPS): # TODO For quick testing purposes\n",
    "        print(f\"####\\n{GROUP.capitalize()} \", group)\n",
    "        sample = group_data_map[group]\n",
    "        if len(sample) == 0:\n",
    "            continue\n",
    "        group_cdfs_df = all_cdfs_df.copy()\n",
    "        master_df.loc[group, 'kstest_stat_initial'] = 1\n",
    "\n",
    "        for j in range(len(EXCLUDE_CUTOFFS)):\n",
    "\n",
    "            exclude_cutoff = EXCLUDE_CUTOFFS[j]\n",
    "            exclude_cutoff = int(exclude_cutoff)\n",
    "\n",
    "            if exclude_cutoff == 0:\n",
    "                augmented_var = np.var(group_data_map[group]) \n",
    "            else:\n",
    "                augmented_var = np.var(group_data_map[group][exclude_cutoff:-exclude_cutoff])\n",
    "                if augmented_var == np.var(group_data_map[group]):\n",
    "                    continue \n",
    "                \n",
    "            group_cdfs_df['scale'] = np.clip((augmented_var / group_cdfs_df['variance']), 0,  SCALE_CAP)\n",
    "            ksstats, initial_param, min_stat = gridsearch(sample, all_cdfs, debug=True, scales=group_cdfs_df['scale'])\n",
    "            augmented_var_df.loc[group, f\"kstest_stat_{exclude_cutoff}\"] = min_stat\n",
    "            group_cdfs_df[f'ksstat_exclude_{exclude_cutoff}'] = ksstats\n",
    "            \n",
    "            print(group, exclude_cutoff, min_stat, augmented_var)\n",
    "            \n",
    "            if (min_stat < master_df.loc[group, 'kstest_stat_initial']) and not np.isclose(min_stat, master_df.loc[group, 'kstest_stat_initial'], 1e-6):\n",
    "                inital_min_stat = min_stat\n",
    "                inital_best_param = initial_param\n",
    "                master_df.loc[group, 'kstest_stat_initial'] = min_stat\n",
    "                initial_scale = group_cdfs_df.loc[(group_cdfs_df[\"r\"] ==  initial_param[0]) & (group_cdfs_df[\"eta\"] ==  initial_param[1])][\"scale\"].iloc[0]\n",
    "                master_df.loc[group, 'initial_r'], master_df.loc[group, 'initial_eta'] = initial_param\n",
    "                master_df.loc[group, 'initial_scale'] = initial_scale\n",
    "                master_df.loc[group, 'initial_exclude_cutoff'] = exclude_cutoff\n",
    "                master_df.loc[group, 'best_augmented_var'] = augmented_var\n",
    "        \n",
    "        ksstats = group_cdfs_df[f\"ksstat_exclude_{int(master_df.loc[group, 'initial_exclude_cutoff'])}\"]\n",
    "            \n",
    "        print(f\"Number of samples: {sample.size}, Without approximation : {master_df.loc[group, 'total_samples']}\")\n",
    "        \n",
    "        cutoff = stats.kstwo(n=master_df.loc[group, 'total_samples']).isf(0.05)\n",
    "        master_df.loc[group, 'kstest_stat_cutoff_0.05'] = cutoff\n",
    "\n",
    "        best_scales = np.clip((master_df.loc[group, 'best_augmented_var'] / group_cdfs_df['variance']), 0,  SCALE_CAP)\n",
    "\n",
    "        group_cdfs_df['variance'] = group_cdfs_df['variance'] * best_scales\n",
    "        group_cdfs_df['kurtosis'] = group_cdfs_df['kurtosis'] * best_scales\n",
    "\n",
    "        group_cdfs_df = add_tests_to_df(cdfs_df = group_cdfs_df, group = group, var_kurt_df = master_df, ksstats = ksstats).sort_values(['r', 'eta'])\n",
    "\n",
    "        group_dict = {'r' : group_cdfs_df['r'], 'eta' : group_cdfs_df['eta']}\n",
    "        group_dict.update({f'ksstat_exclude_{exclude_cutoff}' : group_cdfs_df[f'ksstat_exclude_{exclude_cutoff}'] for exclude_cutoff in EXCLUDE_CUTOFFS[:j+1]})\n",
    "        rEtaKsstats_dict[group] = group_dict\n",
    "\n",
    "        cols = ['pass_var', 'pass_kstest', 'pass_kurt']\n",
    "\n",
    "        fig = combo_test_plot(group_cdfs_df, cols, \n",
    "                            plot_name=f\"{GROUP.capitalize()} {group}: {', '.join([col[5:].capitalize() for col in cols])} Exclude:{master_df.loc[group, 'initial_exclude_cutoff']}\", \n",
    "                            target_var = None,\n",
    "                            best_param = inital_best_param,\n",
    "                            best_ksstat=inital_min_stat\n",
    "                            )\n",
    "        \n",
    "        fig.figure.savefig(os.path.join(CWD, \"plots\", f\"full_grid_search_combo_plot_layer{group}.jpg\"), bbox_inches = 'tight', dpi=100)\n",
    "\n",
    "        # Optional: \n",
    "        # Create plots of bootstrapped variance and kurtosis for varying confidence intervals\n",
    "        # fig_var = create_ci_scatter_plot(group_cdfs_df, var_values_dict, metric='variance', group=group)\n",
    "        # fig_kurt = create_ci_scatter_plot(group_cdfs_df, kurt_values_dict, metric='kurtosis', group=group)\n",
    "\n",
    "        # fig_var.savefig(os.path.join(CWD, \"plots\", f\"ci_scatter_variance_{GROUP}_{group}_bootstrap{n_bootstrap}.jpg\"), bbox_inches='tight')\n",
    "        # plt.close(fig_var)\n",
    "        # fig_kurt.savefig(os.path.join(CWD, \"plots\", f\"ci_scatter_variance_{GROUP}_{group}_bootstrap{n_bootstrap}.jpg\"), bbox_inches='tight')\n",
    "        # plt.close(fig_kurt)\n",
    "\n",
    "    master_df.to_csv(master_df_combo_path)\n",
    "    pd.to_pickle(rEtaKsstats_dict, rEtaKsstats_dict_path)\n",
    "    augmented_var_df.to_csv(augmented_var_df_path)\n",
    "\n",
    "augmented_var_df = pd.read_csv(augmented_var_df_path, index_col=GROUP)\n",
    "master_df = pd.read_csv(master_df_combo_path, index_col=GROUP)\n",
    "rEtaKsstats_dict = pd.read_pickle(rEtaKsstats_dict_path)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['kstest_stat_best'] = master_df['kstest_stat_initial']\n",
    "master_df[f'best_r'] = master_df['initial_r']\n",
    "master_df[f'best_eta'] = master_df['initial_eta']\n",
    "master_df[f'best_scale'] = master_df['initial_scale']\n",
    "master_df[f'best_exclude_cutoff'] = master_df['initial_exclude_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_optimized_path = Path(os.path.join(CWD, \"CSVs\", 'master_df_optimized.csv'))\n",
    "\n",
    "NUM_ITERS = dict(zip(GROUPS, [1]*len(GROUPS))) # By default, does one iteration per group\n",
    "NUM_SAMPLES_OPTIMIZE = 2000\n",
    "EXCLUDE_CUTOFFS_OPTIMIZE = np.arange(100, -101, -25)\n",
    "\n",
    "if RERUN or not master_df_optimized_path.exists():\n",
    "\n",
    "    if SKIP_OPTIMIZE_STEP:\n",
    "\n",
    "        master_df['kstest_stat_best'] = master_df['kstest_stat_initial']\n",
    "        master_df[f'best_r'] = master_df['initial_r']\n",
    "        master_df[f'best_eta'] = master_df['initial_eta']\n",
    "        master_df[f'best_scale'] = master_df['initial_scale']\n",
    "        master_df[f'best_exclude_cutoff'] = master_df['initial_exclude_cutoff']\n",
    "\n",
    "    else:\n",
    "        for group in GROUPS: # TODO For quick testing purposes\n",
    "            print(f\"{GROUP.capitalize()} {group}\")\n",
    "            master_df.loc[group, 'kstest_stat_best'] = master_df.loc[group, 'kstest_stat_initial']\n",
    "            master_df.loc[group, 'best_r'] = master_df.loc[group, 'initial_r']\n",
    "            master_df.loc[group, 'best_eta'] = master_df.loc[group, 'initial_eta']\n",
    "            master_df.loc[group, 'best_scale'] = master_df.loc[group, 'initial_scale']\n",
    "\n",
    "            sample = group_data_map[group]\n",
    "            if len(sample) == 0:\n",
    "                continue\n",
    "            initial_r, initial_eta = master_df.loc[group, 'initial_r'], master_df.loc[group, f'initial_eta']\n",
    "            eps = 0.5\n",
    "            group_cdfs_df = fine_cdfs_df[(np.abs(master_df.loc[group, 'initial_r'] - fine_cdfs_df['r']) < eps) & \n",
    "                                        (np.abs(master_df.loc[group, 'initial_eta'] - fine_cdfs_df['eta']) < eps)].reset_index(drop=True)\n",
    "            group_cdfs = {x[0]:x[1] for x in group_cdfs_df['(r,eta),cdf']}\n",
    "\n",
    "            for adjust_exclude_cutoff in EXCLUDE_CUTOFFS_OPTIMIZE: # -50, -25, 0, 25, 50\n",
    "                \n",
    "                exclude_cutoff = int(max(0, adjust_exclude_cutoff + master_df.loc[group, 'initial_exclude_cutoff']))\n",
    "                \n",
    "                if exclude_cutoff == 0:\n",
    "                    augmented_var = np.var(group_data_map[group])\n",
    "                else:\n",
    "                    augmented_var = np.var(group_data_map[group][exclude_cutoff:-exclude_cutoff])                       \n",
    "\n",
    "                group_cdfs_df['scale'] = np.clip((augmented_var / group_cdfs_df['variance']), 0,  SCALE_CAP)\n",
    "                ksstats, initial_param, min_stat = gridsearch(sample, group_cdfs, debug=True, scales=group_cdfs_df['scale'])\n",
    "                \n",
    "                cutoff_label = generate_cutoff_label(adjust_exclude_cutoff)\n",
    "\n",
    "                augmented_var_df.loc[group, f\"kstest_stat_{cutoff_label}\"] = min_stat\n",
    "                group_cdfs_df[f'ksstat_exclude_{cutoff_label}'] = ksstats\n",
    "                \n",
    "                print(f\"{GROUP} {group}, {master_df.loc[group, 'initial_exclude_cutoff']} + {adjust_exclude_cutoff} = {exclude_cutoff}, ksstat: {min_stat}, var: {augmented_var}\")\n",
    "                best_adjust = 0\n",
    "\n",
    "                if min_stat < master_df.loc[group, 'kstest_stat_best']:\n",
    "\n",
    "                    inital_min_stat = min_stat\n",
    "                    inital_best_param = initial_param\n",
    "                    master_df.loc[group, 'kstest_stat_best'] = min_stat\n",
    "                    initial_scale = group_cdfs_df.loc[(group_cdfs_df[\"r\"] ==  initial_param[0]) & (group_cdfs_df[\"eta\"] ==  initial_param[1])][\"scale\"].iloc[0]\n",
    "                    master_df.loc[group, 'best_r'], master_df.loc[group, 'best_eta'] = initial_param\n",
    "                    master_df.loc[group, 'best_scale'] = initial_scale\n",
    "                    master_df.loc[group, 'best_exclude_cutoff'] = exclude_cutoff\n",
    "                    master_df.loc[group, 'best_augmented_var'] = augmented_var\n",
    "                    augmented_var_df.loc[group, 'best_augmented_var'] = augmented_var\n",
    "                    best_adjust = adjust_exclude_cutoff\n",
    "\n",
    "                if exclude_cutoff == 0:\n",
    "                    break\n",
    "\n",
    "            cutoff_label = generate_cutoff_label(best_adjust)\n",
    "            ksstats = group_cdfs_df[f\"ksstat_exclude_{cutoff_label}\"]\n",
    "            \n",
    "            print(f\"Number of samples: {sample.size}, Without approximation : {master_df.loc[group, 'total_samples']}\")\n",
    "            \n",
    "            cutoff = stats.kstwo(n=master_df.loc[group, 'total_samples']).isf(0.05)\n",
    "            master_df.loc[group, 'kstest_stat_cutoff_0.05'] = cutoff\n",
    "\n",
    "            best_scales = np.clip((master_df.loc[group, 'best_augmented_var'] / group_cdfs_df['variance']), 0,  SCALE_CAP)\n",
    "\n",
    "            group_cdfs_df['variance'] = group_cdfs_df['variance'] * best_scales\n",
    "            group_cdfs_df['kurtosis'] = group_cdfs_df['kurtosis'] * best_scales\n",
    "\n",
    "            group_cdfs_df = add_tests_to_df(cdfs_df = group_cdfs_df, group = group, var_kurt_df = master_df, ksstats = ksstats).sort_values(['r', 'eta'])\n",
    "\n",
    "            group_dict = rEtaKsstats_dict[group]\n",
    "            group_dict.update({'r_optimize' : group_cdfs_df['r'], 'eta_optimize' : group_cdfs_df['eta'], f'ksstat_exclude_{cutoff_label}' : ksstats})\n",
    "            rEtaKsstats_dict[group] = group_dict\n",
    "\n",
    "            cols = ['pass_var', 'pass_kstest', 'pass_kurt']\n",
    "            fig = combo_test_plot(group_cdfs_df, cols, \n",
    "                                plot_name=f\"{GROUP.capitalize()} {group} zoomed in: {', '.join([col[5:].capitalize() for col in cols])}  Exclude:{master_df.loc[group, 'best_exclude_cutoff']}\",\n",
    "                                best_param=(master_df.loc[group, 'best_r'], master_df.loc[group, f'best_eta']),\n",
    "                                best_ksstat=master_df.loc[group, 'kstest_stat_best'])\n",
    "            fig.figure.savefig(os.path.join(CWD, \"plots\", f\"optimized_full_grid_search_combo_plot_layer{group}.jpg\"), bbox_inches = 'tight', dpi=100)\n",
    "            \n",
    "    master_df['n_pval_0.05'] = master_df.apply(lambda row : find_n_fixed_pval_stat(row.loc['kstest_stat_best'], row.loc['total_samples']), axis = 1)    \n",
    "    master_df[['total_samples', 'initial_r', 'initial_eta', 'kstest_stat_initial', 'initial_exclude_cutoff', 'best_r', 'best_eta', 'best_scale', 'kstest_stat_best', 'best_exclude_cutoff', 'n_pval_0.05']].to_csv(os.path.join(CWD, \"CSVs\", 'optimized_params.csv'))\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", 'master_df_optimized.csv'))\n",
    "    augmented_var_df.to_csv(augmented_var_df_path)\n",
    "    pd.to_pickle(rEtaKsstats_dict, rEtaKsstats_dict_path)\n",
    "\n",
    "rEtaKsstats_dict = pd.read_pickle(rEtaKsstats_dict_path)\n",
    "master_df = pd.read_csv(master_df_optimized_path, index_col = GROUP)\n",
    "augmented_var_df = pd.read_csv(augmented_var_df_path, index_col = GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Empirical and Computed CDF/PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in GROUPS:\n",
    "    group_info = master_df.loc[group]\n",
    "    best_r = group_info['best_r']\n",
    "    best_eta = group_info['best_eta']\n",
    "    best_scale = group_info['best_scale']\n",
    "    fig = visualize_cdf_pdf(sample = group_data_map[group], \n",
    "                    params = (best_r, best_eta, best_scale), \n",
    "                    log_scale = True,\n",
    "                    group = group)\n",
    "    fig.savefig(os.path.join(CWD, \"plots\", f'compare_cdf_pdf_layer_{group}.jpg'), bbox_inches = 'tight', dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with Gaussian and Laplace Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_func(sample, distro, *args, n_samples=200):\n",
    "    if distro == 'gaussian' or distro == 'normal':\n",
    "        def var_func(var):\n",
    "            cdf = scipy.stats.norm(scale=var).cdf\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return var_func\n",
    "    elif distro == 'laplace':\n",
    "        def var_func(var):\n",
    "            cdf = scipy.stats.laplace(scale=var).cdf\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return var_func\n",
    "    elif distro == 't':\n",
    "        def var_func(var):\n",
    "            cdf = scipy.stats.t(df=2, scale=var).cdf\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return var_func\n",
    "    elif distro == 'prior_r':\n",
    "        eta = args[0]\n",
    "        def r_func(r):\n",
    "            cdf = compute_prior_cdf(r, eta, n_samples=n_samples)\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return r_func\n",
    "    elif distro == 'prior_eta':\n",
    "        r = args[0]\n",
    "        def eta_func(eta):\n",
    "            cdf = compute_prior_cdf(r, eta, n_samples=n_samples)\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return eta_func\n",
    "    elif distro == 'prior':\n",
    "        def r_eta_func(params):\n",
    "            r = params[0]\n",
    "            eta = params[1]\n",
    "            cdf = compute_prior_cdf(r, eta, n_samples=n_samples, debug=False)\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return r_eta_func\n",
    "    elif distro == 'prior_with_scale':\n",
    "        def r_eta_scale_func(params):\n",
    "            r = params[0]\n",
    "            eta = params[1]\n",
    "            scale = params[2]\n",
    "            print(r, eta, scale)\n",
    "            cdf = compute_prior_cdf(r = r, eta = eta, n_samples=n_samples, debug=False)\n",
    "            return compute_ksstat(sample/ np.sqrt(scale), cdf)\n",
    "        return r_eta_scale_func\n",
    "\n",
    "    print(\"Please enter a valid argument for `distro`: 'gaussian', 'laplace', 'prior_r', 'prior_eta', 'prior','prior_with_scale', 't'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_path = Path(os.path.join(CWD, \"CSVs\", 'master_df.csv'))\n",
    "\n",
    "if RERUN or not master_df_path.exists():\n",
    "    upper_bound = int(1e6)\n",
    "    for group in GROUPS:\n",
    "        if len(group_data_map[group]) == 0:\n",
    "            continue\n",
    "        norm_result = scipy.optimize.minimize_scalar(generate_func(group_data_map[group], 'gaussian'), method = 'bounded', bounds = (0, upper_bound))\n",
    "        laplace_result = scipy.optimize.minimize_scalar(generate_func(group_data_map[group], 'laplace'), method = 'bounded', bounds = (0, upper_bound))\n",
    "        t_result = scipy.optimize.minimize_scalar(generate_func(group_data_map[group], 't'), method = 'bounded', bounds = (0, upper_bound))\n",
    "        \n",
    "        master_df.loc[group, 'param_gaussian'] = round_to_sigfigs(norm_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_gaussian'] = round_to_sigfigs(norm_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_gaussian'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_gaussian']), 6)\n",
    "\n",
    "        master_df.loc[group, 'param_laplace'] = round_to_sigfigs(laplace_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_laplace'] = round_to_sigfigs(laplace_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_laplace'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_laplace']), 6)\n",
    "\n",
    "        master_df.loc[group, 'param_laplace'] = round_to_sigfigs(laplace_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_laplace'] = round_to_sigfigs(laplace_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_laplace'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_laplace']), 6) \n",
    "\n",
    "        master_df.loc[group, 'param_t'] = round_to_sigfigs(t_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_t'] = round_to_sigfigs(t_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_t'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_t']), 6) \n",
    "\n",
    "        master_df.loc[group, 'kstest_pval_gengamma'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_best']))\n",
    "\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", 'master_df.csv'))\n",
    "    \n",
    "master_df = pd.read_csv(os.path.join(CWD, \"CSVs\", 'master_df.csv'), index_col = GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "master_df = pd.read_csv(os.path.join(os.getcwd(), \"CSVs\", 'master_df.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.filter(regex=\"kstest_stat.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.filter(regex=\"best.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
